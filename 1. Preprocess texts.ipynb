{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8e2046",
   "metadata": {},
   "source": [
    "# Process texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830358b",
   "metadata": {},
   "source": [
    "В цьому ноутбуці ми завантажимо,  перекладемо, почистимо тексти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e188f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BHaiov01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "### Clean texts\n",
    "\n",
    "import re\n",
    "\n",
    "import demoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import cld2\n",
    "from numpy import isin\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from lemmatization import process, process_pipeline\n",
    "\n",
    "\n",
    "# demoji.download_codes()\n",
    "\n",
    "whitespace = re.compile(\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "\n",
    "def simple_tokenize(text, max_shrink=False):\n",
    "    \"\"\"\n",
    "    Tokenizer for sentiment analysis of mentions (MOSTLY Facebook text)\n",
    "    \"\"\"\n",
    "#     delete urls\n",
    "    clean_text = re.sub(r'http\\S+', ' ', text)\n",
    "#     delete everything in brackets -- <>\n",
    "    clean_text = re.sub(r\"</?\\w*>\", \" \", clean_text)\n",
    "#     delete\n",
    "    for c in [\"(\", \")\", \"!\", \"?\", r\"-\", r\"*\", \".\", \",\", r'\"']:\n",
    "        clean_text = re.sub(\"[%s]+\" % c, c, clean_text)\n",
    "\n",
    "#     delete hashtags\n",
    "    clean_text = re.sub(r\"#\\w*|№\\w*\", \" \", clean_text)\n",
    "    \n",
    "#     delete complex model names\n",
    "    clean_text = re.sub(r\"\\b\\S+[0-9]+\\S+\\b\", \" \", clean_text)\n",
    "\n",
    "    if clean_text == '':\n",
    "        return \"NULL\"\n",
    "\n",
    "    clean_text = clean_text.replace(\"\\n\", ' ')\n",
    "    clean_text = clean_text.replace(\"\\b\", \" \")\n",
    "    clean_text = clean_text.replace(\"=\", \" \")\n",
    "    clean_text = clean_text.replace(\"+\", \" \")\n",
    "\n",
    "    # add spaces around punctuation\n",
    "    chars = string.punctuation\n",
    "    chars = chars.replace(\"`\", \"\")\n",
    "    chars = chars.replace(\"'\", \"\")\n",
    "    chars += \"«»\\\"“”‘’.?!…,:;\"\n",
    "\n",
    "    for c in chars:\n",
    "        clean_text = clean_text.replace(c, ' ')\n",
    "\n",
    "#     delete emojis\n",
    "    clean_text = demoji.replace(clean_text)\n",
    "#     remove digits\n",
    "    clean_text = ''.join(ch for ch in clean_text if ch not in string.digits)\n",
    "#     set to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    if clean_text.replace(' ', '') == '':\n",
    "        return \"NULL\"\n",
    "\n",
    "    only_letters = re.sub(f\"[0-9]|[{chars}]|\\s\", \"\", clean_text)\n",
    "\n",
    "    if only_letters == '' or len(only_letters) == 0:\n",
    "        return \"NULL\"\n",
    "    else:\n",
    "        if max_shrink:\n",
    "            return re.sub(f\"[0-9]|[{chars}]\", \"\", clean_text)\n",
    "        else:\n",
    "            return clean_text\n",
    "\n",
    "\n",
    "def squeeze_whitespace(input_):\n",
    "    return whitespace.sub(\" \", input_).strip()\n",
    "\n",
    "\n",
    "def tokenize(text, max_shrink=False):\n",
    "    \"\"\"\n",
    "    Function refers to splitting up a larger body of text into words\n",
    "    text: parameter takes sentence\n",
    "    max_shrink: leaves only letters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert isinstance(text, str)\n",
    "    except AssertionError:\n",
    "        return None\n",
    "    else:\n",
    "        return squeeze_whitespace(simple_tokenize(text, max_shrink))\n",
    "\n",
    "\n",
    "def predict_languages_with_cld2(text):\n",
    "    \"\"\"\n",
    "    # BROKEN LOGIC\n",
    "    text: str\n",
    "    \"\"\"\n",
    "    assert isinstance(text, str)\n",
    "\n",
    "    try:\n",
    "        output = cld2.detect(text)\n",
    "    except ValueError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    pred_lang = output.details[0].language_name\n",
    "    if pred_lang == \"RUSSIAN\":\n",
    "        return \"ru\"\n",
    "    elif pred_lang == \"UKRAINIAN\":\n",
    "        return \"uk\"\n",
    "    elif pred_lang == \"ENGLISH\":\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def main(df):\n",
    "#     cleaning\n",
    "    df[\"clean_text\"] = df.text.apply(lambda x: tokenize(x))\n",
    "\n",
    "#     remove empty texts\n",
    "    print(f\"Total number of empty texts is -- {df[df.clean_text == 'NULL'].shape[0]}\")\n",
    "    df = df[df.clean_text != \"NULL\"]\n",
    "    \n",
    "#     frop NAs from the dataframe\n",
    "    df.dropna(subset=[\"clean_text\"], inplace=True)\n",
    "\n",
    "#     Predict languages\n",
    "    df['lang'] = df.clean_text.apply(lambda x: predict_languages_with_cld2(x))\n",
    "\n",
    "    df = df[isin(df.lang.tolist(), [\"ru\", \"uk\"])]\n",
    "\n",
    "    ### Translate\n",
    "    # red translated texts\n",
    "    df_rus = pd.read_excel(r\"C:\\Users\\BHaiov01\\Documents\\Projects\\Analysis\\gorenje_2022\\translations\\november\\gorenje_ready_for_translation.xlsx\", \n",
    "                           index_col=0)\n",
    "    print(f\"Loaded {df_rus.shape[0]} translated rows\")\n",
    "\n",
    "#     leave only indexes that are needed\n",
    "    df_rus = df_rus[isin(df_rus.index, df.index)]\n",
    "    df = df[isin(df.index, df_rus.index)]\n",
    "\n",
    "#     clean translated texts\n",
    "    df_rus[\"clean_rus\"] = df_rus.translated_text.apply(lambda x: tokenize(x))\n",
    "\n",
    "    ### Remove stopwords\n",
    "\n",
    "    with open(\"helpdata/stop_words_russian.txt\", \"r\") as f:\n",
    "        stop_words = [w.replace(\"\\n\", \"\") for w in f.readlines()]\n",
    "\n",
    "    c = 0\n",
    "    for w in stopwords.words(\"russian\"):\n",
    "        if w in stop_words:\n",
    "            c += 1\n",
    "        else:\n",
    "            stop_words.append(w)\n",
    "    print(\"Total number of stopwords is\", len(stop_words))\n",
    "\n",
    "    df_rus[\"clean_rus_no_stopwords\"] = df_rus.clean_rus.apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n",
    "    \n",
    "    # lets remove russian names\n",
    "\n",
    "    russian_names = pd.read_csv(\"russian_names.csv\", sep=';')\n",
    "    russian_names.drop(columns=['ID', 'Sex', 'PeoplesCount', 'WhenPeoplesCount', 'Source'], inplace=True)\n",
    "    russian_names = russian_names.Name.tolist()\n",
    "    print(\"Loaded\", len(russian_names), \"unique russian names\")\n",
    "\n",
    "    df_rus[\"clean_rus_no_stopwords\"] = df_rus.clean_rus_no_stopwords.apply(lambda x: \" \".join([w for w in x.split() if w not in russian_names]))\n",
    "    \n",
    "    df_rus[\"index\"] = df_rus.index\n",
    "\n",
    "    ### Lemmatization\n",
    "    lemmatized = {}\n",
    "\n",
    "    # {text_index : [list of lemmatized words], ...}\n",
    "    for index, text in zip(df_rus.index, df_rus.clean_rus_no_stopwords.tolist()):\n",
    "        lemmatized_words = process(process_pipeline, text, keep_pos=True)\n",
    "        lemmatized[index] = [w for w in lemmatized_words if w.split(\"_\")[0] not in stop_words and \\\n",
    "                             w.split(\"_\")[0] not in russian_names]\n",
    "        \n",
    "    df_rus[\"lemmatized\"] = None\n",
    "\n",
    "    for index in df_rus.index:\n",
    "        df_rus.loc[index, \"lemmatized\"] = \" \".join(lemmatized[index])\n",
    "        \n",
    "    return df, df_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65eec44",
   "metadata": {},
   "source": [
    "##### Завантажимо дані та викинемо дублікати"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df372eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32071 rows\n",
      "Number of unique rows is -- 22263\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"november_raw_data.xlsx\", index_col=0)\n",
    "\n",
    "# df.head()\n",
    "print(f\"Loaded {df.shape[0]} rows\")\n",
    "\n",
    "df.drop(columns=['Страна',\n",
    "       'Регион', 'Город', 'Уровень издания', 'Рубрика', 'Язык',\n",
    "       'Дата постановки', 'Ссылка', 'Время начала (ТВ)',\n",
    "       'Время окончания (ТВ)', 'Аудитория/тираж', 'Аннотация', 'Дубли', 'Код',\n",
    "       'Cредняя посещаемость', 'Аудитория автора', 'Аудитория сообщества'], inplace=True)\n",
    "df.columns = [\"title\", \"text\", \"source_type\", \"source\", \"date\"]\n",
    "\n",
    "# drop duplicated rows\n",
    "df.drop_duplicates(subset=[\"text\"], keep=\"first\", inplace=True)\n",
    "print(f\"Number of unique rows is -- {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306df09f",
   "metadata": {},
   "source": [
    "##### ф-ція для порівняння тексту та регулярного виразу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d883a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_mention(text):\n",
    "    \"\"\"\n",
    "    Function checks if there is a mention of tags in the text\n",
    "    \"\"\"\n",
    "    if re.findall(req.lower(), text.lower()) == []:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "contains_mention_MP = np.vectorize(contains_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1e4fe",
   "metadata": {},
   "source": [
    "##### Оберемо тільки ті рядочки, які відповідають запиту для побутової техніки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812b70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts related to home appliances -- 7311\n"
     ]
    }
   ],
   "source": [
    "from requests_finder import load_request\n",
    "\n",
    "req = load_request(fname=\"request.txt\")\n",
    "df[\"home_appliances\"] = contains_mention_MP(df.text.values)\n",
    "print(\"Number of texts related to home appliances --\", df[df.home_appliances == 1].shape[0])\n",
    "df = df[df.home_appliances == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49d50d",
   "metadata": {},
   "source": [
    "##### Проставимо 1-й набір тегів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6dffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/13 | НЕРЕЛЕВАНТ\n",
      "2/13 | Продаж нової техніки\n",
      "3/13 | Продаж Б/У\n",
      "4/13 | Ремонт/Поломки/Деталі\n",
      "5/13 | Послуги\n",
      "6/13 | Огляд\n",
      "7/13 | Розіграші\n",
      "8/13 | Продажа(здача) квартир з подутовою технікою\n",
      "9/13 | Корп активність в компанії\n",
      "10/13 | Софти\n",
      "11/13 | Відгуки в інтернет-магазинах (розширити тематику)\n",
      "12/13 | Вакансії/робота\n",
      "13/13 | інші електронні пристрої\n"
     ]
    }
   ],
   "source": [
    "all_themes = pd.read_excel(\"tags_gorenje.xlsx\", sheet_name=\"MAIN_1\")\n",
    "\n",
    "for index, row in all_themes.iterrows():\n",
    "    req = load_request(request=row[\"Примеры ключевых слов\"])\n",
    "    theme = row[\"Категории\"]\n",
    "    print(f\"{index + 1}/{all_themes.shape[0]} | {theme}\")\n",
    "    df[f\"MAIN_1: {theme}\"] = contains_mention_MP(df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f2bbb",
   "metadata": {},
   "source": [
    "##### Проставимо 2-й набір тегів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe84cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/12 | Холодильник\n",
      "2/12 | Пральна машина\n",
      "3/12 | Сушка\n",
      "4/12 | Посудомийка\n",
      "5/12 | Морозильна камера\n",
      "6/12 | Духовка\n",
      "7/12 | Варочні поверхні\n",
      "8/12 | Витяжка\n",
      "9/12 | Мікрохвильовка\n",
      "10/12 | Кондиціонер\n",
      "11/12 | Пилосос\n",
      "12/12 | Бойлер\n"
     ]
    }
   ],
   "source": [
    "all_themes_2 = pd.read_excel(\"tags_gorenje.xlsx\", sheet_name=\"MAIN_2\")\n",
    "\n",
    "for index, row in all_themes_2.iterrows():\n",
    "    req = load_request(request=row[\"Примеры ключевых слов\"])\n",
    "    theme = row[\"Категории\"]\n",
    "    print(f\"{index + 1}/{all_themes_2.shape[0]} | {theme}\")\n",
    "    df[f\"MAIN_2: {theme}\"] = contains_mention_MP(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a8dc986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\\\\bстирал\\\\w*.{1,5}\\\\bмашин\\\\w*|\\\\bмашин\\\\w*.{1,5}\\\\bстирал\\\\w*|\\\\bпральн\\\\w*.{1,5}\\\\bмашин\\\\w*|\\\\bмашин\\\\w*.{1,5}\\\\bпральн\\\\w*|\\\\bпранн\\\\w*.{1,5}\\\\bмашин\\\\w*|\\\\bмашин\\\\w*.{1,5}\\\\bпранн\\\\w*|\\\\bстирк\\\\w*.{1,5}\\\\bмашин\\\\w*|\\\\bмашин\\\\w*.{1,5}\\\\bстирк\\\\w*|\\\\bстиралк\\\\w*|\\\\bпрал\\\\w*)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_request(request=all_themes_2.loc[1, \"Примеры ключевых слов\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72148a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((стирал* OR пральн* OR пранн* OR стирк*) /5 машин*) OR стиралк*  OR прал*'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_themes_2.loc[1, \"Примеры ключевых слов\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8f860",
   "metadata": {},
   "source": [
    "##### Обробимо тексти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e18476b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of empty texts is -- 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-7f800740aab5>:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(subset=[\"clean_text\"], inplace=True)\n",
      "<ipython-input-39-7f800740aab5>:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lang'] = df.clean_text.apply(lambda x: predict_languages_with_cld2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29815 translated rows\n",
      "Total number of stopwords is 572\n",
      "Loaded 51529 unique russian names\n"
     ]
    }
   ],
   "source": [
    "df_processed, df_rus = main(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466b8e2",
   "metadata": {},
   "source": [
    "##### Додаткова перевірка обробки\n",
    "\n",
    "Перевіряються короткі слова. Якщо слово менше або рівне 3-х символів та не є словом із головного запиту, ми його видаляємо "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "830ff458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "from requests_finder import load_request\n",
    "import re\n",
    "\n",
    "def decompose_initial_request(path_to_req):\n",
    "    with open(path_to_req, \"r\", encoding=\"utf8\") as f:\n",
    "              req = f.readlines()[0]\n",
    "    \n",
    "    new_req_list = []\n",
    "    req = req.replace(\"(\", \"\")\n",
    "    req = req.replace(\")\", \"\")\n",
    "    req = req.replace(\"/\", \"\")\n",
    "    req = req.replace(\"AND\", \"\")\n",
    "    req = req.replace(\"OR\", \"\")\n",
    "    for digit in digits:\n",
    "        req = req.replace(str(digit), \"\")\n",
    "    \n",
    "    for t in req.split():\n",
    "        new_req_list.append(t)\n",
    "        \n",
    "    new_req = \" OR \".join(new_req_list)\n",
    "    \n",
    "    return new_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4697db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of vocab is 28241\n",
      "692 words were popped\n",
      "Data edited and saved\n"
     ]
    }
   ],
   "source": [
    "WORD_MIN_LENGTH = 2\n",
    "\n",
    "lemmatized = {}\n",
    "for index in df_rus.index:\n",
    "    lemmatized[index] = df_rus.loc[index, \"lemmatized\"].split(\" \")\n",
    "\n",
    "vocab = []\n",
    "for tokens in lemmatized.values():\n",
    "    for word in tokens:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "print(f\"Total length of vocab is {len(vocab)}\")\n",
    "\n",
    "req = decompose_initial_request(path_to_req=\"request.txt\")\n",
    "req = load_request(request=req)\n",
    "\n",
    "popped_words = []\n",
    "for word in vocab:\n",
    "    postag = word.split(\"_\")[1]\n",
    "    word = word.split(\"_\")[0]\n",
    "    if len(word) <= WORD_MIN_LENGTH:\n",
    "        if not contains_mention(word):\n",
    "            popped_words.append(f\"{word}_{postag}\")\n",
    "            vocab.remove(f\"{word}_{postag}\")\n",
    "print(f\"{len(popped_words)} words were popped\")\n",
    "\n",
    "for index, text in lemmatized.items():\n",
    "    text = [w for w in text if w not in popped_words]\n",
    "    lemmatized[index] = text\n",
    "    df_rus.loc[index, \"lemmatized\"] = \" \".join(text)\n",
    "\n",
    "print(\"Data edited and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0bd1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rus[\"lemmatized_with_pos\"] = df_rus[\"lemmatized\"].copy()\n",
    "df_rus[\"lemmatized\"] = df_rus[\"lemmatized\"].apply(lambda x: \" \".join([w.split(\"_\")[0] for w in x.split(\" \")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d918d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>language</th>\n",
       "      <th>clipped_part_correct</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>clean_rus</th>\n",
       "      <th>clean_rus_no_stopwords</th>\n",
       "      <th>index</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_with_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Инверторные кондиционеры | Купить в Москве по ...</td>\n",
       "      <td>инверторные кондиционеры | купить в москве по ...</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>инверторные кондиционеры | купить в москве по ...</td>\n",
       "      <td>инверторные кондиционеры купить в москве по вы...</td>\n",
       "      <td>инверторные кондиционеры купить москве выгодно...</td>\n",
       "      <td>0</td>\n",
       "      <td>инверторный кондиционер покупать москве выгодн...</td>\n",
       "      <td>инверторный_ADJ кондиционер_NOUN покупать_VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пральна машина lg f1296hds1 12 199 грн. https:...</td>\n",
       "      <td>пральна машина lg fhds грн . пральна машина lg...</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стиральная машина LG FHDS HR. Стиральная машин...</td>\n",
       "      <td>стиральная машина lg fhds hr стиральная машина...</td>\n",
       "      <td>стиральная машина lg fhds hr стиральная машина...</td>\n",
       "      <td>1</td>\n",
       "      <td>стиральный машина lg fhds стиральный машина lg...</td>\n",
       "      <td>стиральный_ADJ машина_NOUN lg_X fhds_X стираль...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mention  \\\n",
       "0  Инверторные кондиционеры | Купить в Москве по ...   \n",
       "1  Пральна машина lg f1296hds1 12 199 грн. https:...   \n",
       "\n",
       "                                          clean_text language  \\\n",
       "0  инверторные кондиционеры | купить в москве по ...       ru   \n",
       "1  пральна машина lg fhds грн . пральна машина lg...       uk   \n",
       "\n",
       "   clipped_part_correct                                    translated_text  \\\n",
       "0                   NaN  инверторные кондиционеры | купить в москве по ...   \n",
       "1                   NaN  Стиральная машина LG FHDS HR. Стиральная машин...   \n",
       "\n",
       "                                           clean_rus  \\\n",
       "0  инверторные кондиционеры купить в москве по вы...   \n",
       "1  стиральная машина lg fhds hr стиральная машина...   \n",
       "\n",
       "                              clean_rus_no_stopwords  index  \\\n",
       "0  инверторные кондиционеры купить москве выгодно...      0   \n",
       "1  стиральная машина lg fhds hr стиральная машина...      1   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  инверторный кондиционер покупать москве выгодн...   \n",
       "1  стиральный машина lg fhds стиральный машина lg...   \n",
       "\n",
       "                                 lemmatized_with_pos  \n",
       "0  инверторный_ADJ кондиционер_NOUN покупать_VERB...  \n",
       "1  стиральный_ADJ машина_NOUN lg_X fhds_X стираль...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0793452",
   "metadata": {},
   "source": [
    "##### Збережемо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60e79699",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"home_appliances_with_tags.xlsx\",\n",
    "                        engine='xlsxwriter',\n",
    "                        options={'strings_to_urls': False, 'strings_to_formulas': False})\n",
    "df_processed.to_excel(writer)\n",
    "writer.close()\n",
    "\n",
    "writer = pd.ExcelWriter(\"home_appliances_rus.xlsx\",\n",
    "                        engine='xlsxwriter',\n",
    "                        options={'strings_to_urls': False, 'strings_to_formulas': False})\n",
    "df_rus.to_excel(writer)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
